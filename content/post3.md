
## Abstract

Imbalanced data widely exists in security-prioritized domains, where minority classes often bring a disproportionately high risk. Especially in multi-class scenarios, challenges arise not only from the imbalance between majority and minority classes, but also from the difficulty in separating minority classes and their underrepresentation in the training data. To address these issues, this paper adopts a One-Versus-One decomposition strategy to transform the multi-class problem into multiple binary subtasks. This formulation naturally increases the visibility of minority classes. Furthermore, we propose a large language model-assisted multi-task particle swarm optimization (LLM-MTPSO) algorithm. Each subtask is independently optimized via PSO, while a LLM is leveraged to semantically fuse optimal solutions among similar subtasks. This mechanism facilitates knowledge transfer and refinement at the solution level, enhancing the model’s capacity to recognize minority patterns. Experiments on nine imbalanced datasets show that the test results of our method in terms of imbalanced classification accuracy outperform four classical methods. The ablation experiments indicate that the introduction of LLM promotes information sharing among subtasks, thus improving the algorithm performance.

## Summary of the Proposed Method

The proposed method constructs an LLM-assisted multi-task PSO framework by decomposing the original multi-class imbalanced problem into a set of One-Versus-One (OVO) binary sub-tasks. As illustrated in Fig. 1(a), samples from every class pair are extracted to form their own binary sub-dataset, enabling each sub-task to focus on a single decision boundary and giving minority classes greater visibility in pairwise comparisons. For each sub-task, an independent PSO population is initialized and optimized, where particles update their positions and velocities based on inertia, personal best solutions, and the task-specific global best. Through these updates, PSO explores sub-feature spaces and captures subtle minority-class patterns. When different sub-tasks involve overlapping classes, they are grouped into similar task sets, forming a neighborhood structure that supports knowledge sharing. Each task can then access the historical global best solutions of its neighbors, thereby enriching its own search process.

After an initial round of independent optimization, the method introduces cross-task semantic fusion guided by a large language model. Within each similar-task group, the global best vectors—representing feature-weight solutions in the range ([0,1])—are fed into the LLM, as shown in Fig. 1(b). The LLM is prompted to behave as a feature-selection expert capable of synthesizing these vectors into a new candidate solution that reflects semantic relationships among tasks. This fused candidate is then evaluated, and if its fitness surpasses that of the task’s current global best, it replaces the existing one and is propagated to the population. To manage computational cost, LLM fusion occurs periodically (every ten iterations), while intermediate iterations rely on a lighter crossover mechanism that blends the current global best with a neighbor’s best using a tunable mixing coefficient. This hybrid scheme ensures both efficiency and semantic-rich diversity generation. Throughout the iterative process, the interplay between local PSO optimization, neighbor-based crossover, and LLM-driven fusion broadens the solution space and mitigates premature convergence.

Once all sub-tasks finish optimization, their binary classifiers collectively contribute to the final prediction. A soft-voting mechanism aggregates the outputs of all OVO models, weighting each pairwise decision probabilistically to produce the final multi-class classification result. Overall, the method creates a semantically guided collaborative optimization process: PSO handles task-specific search, similar-task groups support information flow, and the LLM selectively propagates informative patterns, ultimately enhancing the detection of minority classes and improving imbalanced classification performance.
