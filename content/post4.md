
## Abstract

High-dimensional imbalanced data are common in fields such as medical diagnosis and network security. Their complex feature spaces and skewed class distributions pose serious challenges to traditional classification methods. Feature selection is an effective way to alleviate these issues, but existing methods often struggle to balance classification performance and solution diversity. To address this, this paper proposes an angular quantization-guided non-dominated sorting genetic algorithm (AQ-NSGA) for many-objective feature selection. It leverages angular information to guide population evolution and maintain solution diversity by calculating the cosine similarity between new candidates and the population archive, determining whether to retain new individuals to prevent premature convergence. Moreover, AQ-NSGA employs a dynamically adjusted combined evolutionary operator to enhance exploration and convergence at different stages. Experiments on 10 real-world high-dimensional imbalanced datasets demonstrate that AQ-NSGA outperforms two state-of-the-art and two traditional algorithms in terms of balanced accuracy, g-mean, and recall. Ablation studies further confirm the effectiveness of the angular quantization component in improving classification performance.


## Summary of the Proposed Method

The proposed AQ-NSGA method establishes an angular quantization–guided evolutionary framework designed to improve population diversity and predictive robustness when performing class prediction on HD and CI datasets. As shown in Fig.1, angular quantization is embedded throughout initialization, offspring generation, and environmental selection, forming a unified mechanism that regulates similarity among individuals and prevents premature convergence. The algorithm begins by encoding each individual as a bounded real-valued vector. During initialization, the first solution is directly evaluated, whereas each subsequent one computes its cosine similarity with all existing individuals; a dynamic threshold determines whether the new candidate is accepted. This prevents the population from becoming excessively clustered or overly dispersed at the outset, ensuring that meaningful structural diversity is maintained.

After initialization, the evolutionary process proceeds through reproduction guided by angular information. Parents are randomly selected, and offspring are generated using BLX-$\alpha$ crossover, which expands the search interval around parental values to encourage exploration. A hybrid mutation scheme further enhances adaptability: Gaussian mutation introduces broad perturbations in early generations, while non-uniform mutation progressively refines candidate solutions as the algorithm approaches later stages. Each newly generated offspring then undergoes the same angular screening used at initialization, where its cosine similarity with archived individuals is computed; overly similar offspring are filtered out to maintain sufficient diversity in decision space. Cosine similarity is adopted due to its directional nature and robustness in high-dimensional spaces, allowing the algorithm to detect and suppress redundant solutions without relying on magnitude-based distances.

Fitness evaluation for every individual is performed via k-fold cross-validation over selected features, optimizing four objectives simultaneously: accuracy, sensitivity, specificity, and the number of selected features. These metrics collectively define the multi-objective fitness vector for NSGA-based optimization. Following evaluation, environmental selection determines the next generation via fast non-dominated sorting and crowding-distance comparison. Duplicate individuals are removed, and angular information is again incorporated through a penalty term added to the crowding distance. Individuals within the same Pareto front that exhibit high cosine similarity incur penalties, reducing their chances of surviving and thereby improving the spread of the Pareto front. This angular-penalized crowding adjustment ensures that the chosen solutions remain both high-quality and diverse, promoting a well-distributed set of trade-offs across objectives.

To further enhance classification performance, a weighted KNN classifier (KNN2W) is adopted for final prediction. This classifier combines label-frequency weights—which give additional importance to minority classes—with distance-based weights reflecting geometric proximity, mitigating the imbalance issues common in high-dimensional datasets. The number of neighbors is fixed at 5 to balance computational efficiency with generalization performance, consistent with findings from previous work. Altogether, the integration of angular quantization, adaptive evolutionary operators, and an imbalance-aware weighted classifier results in a cohesive framework that preserves high-quality decision boundaries, maintains strong population diversity, and improves classification reliability across complex, high-dimensional domains.
